{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Classifier on Iris Dataset\n",
    "<br>张栋玮 19373703 <a href=\"http://github.com/ZhangDw529/PythonProjects\">Open in GITHUB</a></br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Abstract\n",
    "<br>Naive Bayes is a generative classification algorithm which is based on bayesian theorem and conditional independence assumption. By using bayesian theorem, we can compute posterior probability with prior probabilities and observations. Bayesian theorem is shown as follows.</br>\n",
    "<img src='./pic/bayes.jpg' width=400 height=100 >\n",
    "<br>Let the series of decision actions as ${a_1,a_2,..,a_c}$, the conditional risk $R(a|X)$ of decision action $a_i$ with loss function $\\lambda(a,w)$ can be computed by</br>\n",
    "<img src='./pic/risk.jpg' width=400 height=100>\n",
    "<br>Thus the minimum risk Bayesian decision can be found as</br>\n",
    "<img src='./pic/arg.jpg' width=400 height=100>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Algorithm\n",
    "## 1.1 Data Preprocess\n",
    "Here I load the Iris dataset from sklearn.datasets. Then split it into train set and test set with $test\\_size=0.2,random\\_state=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "def load_data():\n",
    "    iris = datasets.load_iris()\n",
    "    x = iris.data\n",
    "    y = iris.target\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=3)\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bayes Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>This is the python class named bayes. It contains four parts with details in comments.</br>\n",
    "- Initialization and training\n",
    "- Predictions\n",
    "- Computation of accuracy\n",
    "- Data print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayes():\n",
    "\n",
    "    # Initialize and train\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.categories = len(np.unique(y_train))  # 3 classes in the dataset\n",
    "        self.total_col = x_train.shape[1]   # Number of Attributes used\n",
    "        self.partial = []   # Split proportion\n",
    "        self.mean = np.zeros([self.categories,self.total_col]) # Initialization\n",
    "        self.var = np.zeros([self.categories,self.total_col]) \n",
    "\n",
    "        # Compute the proportion, variance and mean of each class       \n",
    "        for i in range(self.categories):\n",
    "            temp = x_train[np.nonzero(i==y_train)] # Select i_th class\n",
    "            self.partial.append(len(temp)/len(x_train))\n",
    "            self.mean[i,:] = np.mean(temp,axis=0,keepdims=True)\n",
    "            self.var[i,:] = np.var(temp,axis=0,keepdims=True)\n",
    "\n",
    "    \n",
    "    # Make predictions\n",
    "    def predict(self, x_test,y_test):\n",
    "        result = []\n",
    "        eps = 1e-10  \n",
    "        for i in x_test:\n",
    "            x = np.tile(i,(3,1))\n",
    "            \n",
    "            # Compute the Gaussian pdf\n",
    "            num = -(x-self.mean+eps)**2\n",
    "            den = 2*self.var+eps\n",
    "            _exp = np.exp(num/den)\n",
    "            # _exp = np.exp(-(x-self.mean)**2/(2*self.var+eps))\n",
    "            \n",
    "            # Compute the posterior possibilities\n",
    "            p = _exp/(np.sqrt(2*np.pi*self.var)+eps)\n",
    "            # Change the possibilities into log() mode\n",
    "            log_p = np.sum(np.log(p),axis=1) \n",
    "            prob = np.log(self.partial)+log_p\n",
    "            result.append(np.argmax(prob))\n",
    "        return result\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    def acc(self, y_test, y_pred):\n",
    "        acc = np.count_nonzero(y_test==y_pred)\n",
    "        return acc/len(y_pred)\n",
    "    \n",
    "    # Print parameters\n",
    "    def printPara(self):\n",
    "        print(f\"The dataset has {self.categories} categories and {self.total_col} attributes.\")\n",
    "        print(\"Proportion of each class:\")\n",
    "        print(self.partial)\n",
    "        print(\"Mean:\")\n",
    "        print(self.mean)\n",
    "        print(\"Var:\")\n",
    "        print(self.var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test\n",
    "<br>In this part, I first computed the parameters of the whole Iris dataset. From the parameters, it is clear that Attribute 3 and 4 are more significant in classification. By doing the following experiments, it shows that using Attribute 3 and 4 to classify the dataset gives a better performance, though it is relative to the split of dataset.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 3 categories and 4 attributes.\n",
      "Proportion of each class:\n",
      "[0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "Mean:\n",
      "[[5.03   3.4325 1.465  0.2375]\n",
      " [5.93   2.7875 4.2675 1.335 ]\n",
      " [6.5525 2.9875 5.5325 2.01  ]]\n",
      "Var:\n",
      "[[0.1011     0.13219375 0.032775   0.01134375]\n",
      " [0.2351     0.10009375 0.21819375 0.040275  ]\n",
      " [0.38749375 0.10309375 0.28519375 0.0649    ]]\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Using all 4 attributes\n",
    "x_train,x_test,y_train,y_test=load_data()\n",
    "TotalAttr = Bayes(x_train,y_train)\n",
    "TotalAttr.printPara()\n",
    "y_pred = TotalAttr.predict(x_test,y_test)\n",
    "acc = TotalAttr.acc(y_test,y_pred)\n",
    "print(f'Accuracy: {acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 3 categories and 2 attributes.\n",
      "Proportion of each class:\n",
      "[0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "Mean:\n",
      "[[1.465  0.2375]\n",
      " [4.2675 1.335 ]\n",
      " [5.5325 2.01  ]]\n",
      "Var:\n",
      "[[0.032775   0.01134375]\n",
      " [0.21819375 0.040275  ]\n",
      " [0.28519375 0.0649    ]]\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Only take attribute 3 and 4 into consideration\n",
    "x_train,x_test,y_train,y_test=load_data()\n",
    "x_train = x_train[:,2:]\n",
    "x_test = x_test[:,2:]\n",
    "Attr34 = Bayes(x_train,y_train)\n",
    "Attr34.printPara()\n",
    "y_pred = Attr34.predict(x_test,y_test)\n",
    "acc = Attr34.acc(y_test,y_pred)\n",
    "print(f'Accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Minimum Risk Bayes\n",
    "<br>In this part, I first implement minimum risk bayes dicision on given data. Then test it on the Iris dataset.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=[-3.9847,-3.5549,-1.2401,-0.9780,-0.7932,-2.8531,-2.7605,-3.7287, \n",
    "-3.5414,-2.2692,-3.4549,-3.0752,-3.9934, -0.9780,-1.5799,-1.4885,\n",
    "-0.7431,-0.4221,-1.1186,-2.3462,-1.0826,-3.4196,-1.3193,-0.8367,\n",
    "-0.6579,-2.9683]\n",
    "w2 =  [2.8792, 0.7932,1.1882,3.0682,4.2532,0.3271,0.9846,2.7648,2.6588]\n",
    "\n",
    "w = [*w1,*w2]\n",
    "y = (np.sign(w)+1)/2    # \n",
    "total = len(w)\n",
    "pw = [0.9,0.1]\n",
    "loss = [[0,1],[6,0]] \n",
    "var = [np.var(w1),np.var(w2)]\n",
    "mean = [np.mean(w1),np.mean(w2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussPdf(x,mean,var):\n",
    "    eps=1e-10\n",
    "    _exp = np.exp(-(x-mean)**2/(2*var+eps))\n",
    "    p = _exp/(np.sqrt(2*np.pi*var)+eps)\n",
    "    return p\n",
    "\n",
    "# x: data  w: class \n",
    "def risk_pred(x):\n",
    "    den = GaussPdf(x,mean[0],var[0])+GaussPdf(x,mean[1],var[1])\n",
    "    num = [loss[0][1]*pw[1]*GaussPdf(x,mean[1],var[1]), loss[1][0]*pw[0]*GaussPdf(x,mean[0],var[0])]\n",
    "    risk = num/den\n",
    "    prediction = np.argmin(risk)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "r_pred = []\n",
    "for i in w :\n",
    "    pred = risk_pred(i)\n",
    "    r_pred.append(pred)\n",
    "correct = np.count_nonzero(r_pred==y)\n",
    "acc = correct/total\n",
    "print(f\"Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "<br>[1] Statistical Pattern Recognition Lab, SASEE</br>\n",
    "<br>[2] <a href=\"https://blog.csdn.net/Happy_change/article/details/117226036\">朴素贝叶斯算法</a></br>\n",
    "<br>[3] <a href=\"https://blog.csdn.net/weixin_46302044/article/details/117399359\">朴素贝叶斯处理鸢尾花数据集分类</a></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
