{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(_BaseNB):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    priors : array-like of shape (n_classes,)\n",
    "        Prior probabilities of the classes. If specified, the priors are not\n",
    "        adjusted according to the data.\n",
    "    var_smoothing : float, default=1e-9\n",
    "        Portion of the largest variance of all features that is added to\n",
    "        variances for calculation stability.\n",
    "        .. versionadded:: 0.20\n",
    "    Attributes\n",
    "    ----------\n",
    "    class_count_ : ndarray of shape (n_classes,)\n",
    "        number of training samples observed in each class.\n",
    "    class_prior_ : ndarray of shape (n_classes,)\n",
    "        probability of each class.\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        class labels known to the classifier.\n",
    "    epsilon_ : float\n",
    "        absolute additive value to variances.\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "        .. versionadded:: 0.24\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "        .. versionadded:: 1.0\n",
    "    sigma_ : ndarray of shape (n_classes, n_features)\n",
    "        Variance of each feature per class.\n",
    "        .. deprecated:: 1.0\n",
    "           `sigma_` is deprecated in 1.0 and will be removed in 1.2.\n",
    "           Use `var_` instead.\n",
    "    var_ : ndarray of shape (n_classes, n_features)\n",
    "        Variance of each feature per class.\n",
    "        .. versionadded:: 1.0\n",
    "    theta_ : ndarray of shape (n_classes, n_features)\n",
    "        mean of each feature per class.\n",
    "    See Also\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "    >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "    >>> from sklearn.naive_bayes import GaussianNB\n",
    "    >>> clf = GaussianNB()\n",
    "    >>> clf.fit(X, Y)\n",
    "    GaussianNB()\n",
    "    >>> print(clf.predict([[-0.8, -1]]))\n",
    "    [1]\n",
    "    >>> clf_pf = GaussianNB()\n",
    "    >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "    GaussianNB()\n",
    "    >>> print(clf_pf.predict([[-0.8, -1]]))\n",
    "    [1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, priors=None, var_smoothing=1e-9):\n",
    "        self.priors = priors\n",
    "        self.var_smoothing = var_smoothing\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Gaussian Naive Bayes according to X, y.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training vectors, where `n_samples` is the number of samples\n",
    "            and `n_features` is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "            .. versionadded:: 0.17\n",
    "               Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        y = self._validate_data(y=y)\n",
    "        return self._partial_fit(\n",
    "            X, y, np.unique(y), _refit=True, sample_weight=sample_weight\n",
    "        )\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"Validate X, used only in predict* methods.\"\"\"\n",
    "        return self._validate_data(X, reset=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_mean_variance(n_past, mu, var, X, sample_weight=None):\n",
    "        \"\"\"Compute online update of Gaussian mean and variance.\n",
    "        Given starting sample count, mean, and variance, a new set of\n",
    "        points X, and optionally sample weights, return the updated mean and\n",
    "        variance. (NB - each dimension (column) in X is treated as independent\n",
    "        -- you get variance, not covariance).\n",
    "        Can take scalar mean and variance, or vector mean and variance to\n",
    "        simultaneously update a number of independent Gaussians.\n",
    "        See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
    "        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_past : int\n",
    "            Number of samples represented in old mean and variance. If sample\n",
    "            weights were given, this should contain the sum of sample\n",
    "            weights represented in old mean and variance.\n",
    "        mu : array-like of shape (number of Gaussians,)\n",
    "            Means for Gaussians in original set.\n",
    "        var : array-like of shape (number of Gaussians,)\n",
    "            Variances for Gaussians in original set.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        \"\"\"\n",
    "        if X.shape[0] == 0:\n",
    "            return mu, var\n",
    "\n",
    "        # Compute (potentially weighted) mean and variance of new datapoints\n",
    "        if sample_weight is not None:\n",
    "            n_new = float(sample_weight.sum())\n",
    "            new_mu = np.average(X, axis=0, weights=sample_weight)\n",
    "            new_var = np.average((X - new_mu) ** 2, axis=0, weights=sample_weight)\n",
    "        else:\n",
    "            n_new = X.shape[0]\n",
    "            new_var = np.var(X, axis=0)\n",
    "            new_mu = np.mean(X, axis=0)\n",
    "\n",
    "        if n_past == 0:\n",
    "            return new_mu, new_var\n",
    "\n",
    "        n_total = float(n_past + n_new)\n",
    "\n",
    "        # Combine mean of old and new data, taking into consideration\n",
    "        # (weighted) number of observations\n",
    "        total_mu = (n_new * new_mu + n_past * mu) / n_total\n",
    "\n",
    "        # Combine variance of old and new data, taking into consideration\n",
    "        # (weighted) number of observations. This is achieved by combining\n",
    "        # the sum-of-squared-differences (ssd)\n",
    "        old_ssd = n_past * var\n",
    "        new_ssd = n_new * new_var\n",
    "        total_ssd = old_ssd + new_ssd + (n_new * n_past / n_total) * (mu - new_mu) ** 2\n",
    "        total_var = total_ssd / n_total\n",
    "\n",
    "        return total_mu, total_var\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"Incremental fit on a batch of samples.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "            .. versionadded:: 0.17\n",
    "        \"\"\"\n",
    "        return self._partial_fit(\n",
    "            X, y, classes, _refit=False, sample_weight=sample_weight\n",
    "        )\n",
    "\n",
    "    def _partial_fit(self, X, y, classes=None, _refit=False, sample_weight=None):\n",
    "        \"\"\"Actual implementation of Gaussian NB fitting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training vectors, where `n_samples` is the number of samples and\n",
    "            `n_features` is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        classes : array-like of shape (n_classes,), default=None\n",
    "            List of all the classes that can possibly appear in the y vector.\n",
    "            Must be provided at the first call to partial_fit, can be omitted\n",
    "            in subsequent calls.\n",
    "        _refit : bool, default=False\n",
    "            If true, act as though this were the first time we called\n",
    "            _partial_fit (ie, throw away any past fitting and start over).\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        if _refit:\n",
    "            self.classes_ = None\n",
    "\n",
    "        first_call = _check_partial_fit_first_call(self, classes)\n",
    "        X, y = self._validate_data(X, y, reset=first_call)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = _check_sample_weight(sample_weight, X)\n",
    "\n",
    "        # If the ratio of data variance between dimensions is too small, it\n",
    "        # will cause numerical errors. To address this, we artificially\n",
    "        # boost the variance by epsilon, a small fraction of the standard\n",
    "        # deviation of the largest dimension.\n",
    "        self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\n",
    "\n",
    "        if first_call:\n",
    "            # This is the first call to partial_fit:\n",
    "            # initialize various cumulative counters\n",
    "            n_features = X.shape[1]\n",
    "            n_classes = len(self.classes_)\n",
    "            self.theta_ = np.zeros((n_classes, n_features))\n",
    "            self.var_ = np.zeros((n_classes, n_features))\n",
    "\n",
    "            self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "            # Initialise the class prior\n",
    "            # Take into account the priors\n",
    "            if self.priors is not None:\n",
    "                priors = np.asarray(self.priors)\n",
    "                # Check that the provided prior matches the number of classes\n",
    "                if len(priors) != n_classes:\n",
    "                    raise ValueError(\"Number of priors must match number of classes.\")\n",
    "                # Check that the sum is 1\n",
    "                if not np.isclose(priors.sum(), 1.0):\n",
    "                    raise ValueError(\"The sum of the priors should be 1.\")\n",
    "                # Check that the priors are non-negative\n",
    "                if (priors < 0).any():\n",
    "                    raise ValueError(\"Priors must be non-negative.\")\n",
    "                self.class_prior_ = priors\n",
    "            else:\n",
    "                # Initialize the priors to zeros for each class\n",
    "                self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64)\n",
    "        else:\n",
    "            if X.shape[1] != self.theta_.shape[1]:\n",
    "                msg = \"Number of features %d does not match previous data %d.\"\n",
    "                raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))\n",
    "            # Put epsilon back in each time\n",
    "            self.var_[:, :] -= self.epsilon_\n",
    "\n",
    "        classes = self.classes_\n",
    "\n",
    "        unique_y = np.unique(y)\n",
    "        unique_y_in_classes = np.in1d(unique_y, classes)\n",
    "\n",
    "        if not np.all(unique_y_in_classes):\n",
    "            raise ValueError(\n",
    "                \"The target label(s) %s in y do not exist in the initial classes %s\"\n",
    "                % (unique_y[~unique_y_in_classes], classes)\n",
    "            )\n",
    "\n",
    "        for y_i in unique_y:\n",
    "            i = classes.searchsorted(y_i)\n",
    "            X_i = X[y == y_i, :]\n",
    "\n",
    "            if sample_weight is not None:\n",
    "                sw_i = sample_weight[y == y_i]\n",
    "                N_i = sw_i.sum()\n",
    "            else:\n",
    "                sw_i = None\n",
    "                N_i = X_i.shape[0]\n",
    "\n",
    "            new_theta, new_sigma = self._update_mean_variance(\n",
    "                self.class_count_[i], self.theta_[i, :], self.var_[i, :], X_i, sw_i\n",
    "            )\n",
    "\n",
    "            self.theta_[i, :] = new_theta\n",
    "            self.var_[i, :] = new_sigma\n",
    "            self.class_count_[i] += N_i\n",
    "\n",
    "        self.var_[:, :] += self.epsilon_\n",
    "\n",
    "        # Update if only no priors is provided\n",
    "        if self.priors is None:\n",
    "            # Empirical prior, with sample_weight taken into account\n",
    "            self.class_prior_ = self.class_count_ / self.class_count_.sum()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        joint_log_likelihood = []\n",
    "        for i in range(np.size(self.classes_)):\n",
    "            jointi = np.log(self.class_prior_[i])\n",
    "            n_ij = -0.5 * np.sum(np.log(2.0 * np.pi * self.var_[i, :]))\n",
    "            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) / (self.var_[i, :]), 1)\n",
    "            joint_log_likelihood.append(jointi + n_ij)\n",
    "\n",
    "        joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "        return joint_log_likelihood\n",
    "\n",
    "    @deprecated(  # type: ignore\n",
    "        \"Attribute `sigma_` was deprecated in 1.0 and will be removed in\"\n",
    "        \"1.2. Use `var_` instead.\"\n",
    "    )\n",
    "    @property\n",
    "    def sigma_(self):\n",
    "        return self.var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27b508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Testset is: 95%\n",
      "The result of predict is: \n",
      "    y_true  y_predict predict_0 predict_1 predict_2\n",
      "0       1          1      0.00      0.95      0.05\n",
      "1       2          2      0.00      0.00      1.00\n",
      "2       2          2      0.00      0.00      1.00\n",
      "3       1          1      0.00      1.00      0.00\n",
      "4       0          0      1.00      0.00      0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "class bayes_model():\n",
    "    def __int__(self):\n",
    "        pass\n",
    "    def load_data(self):\n",
    "        data = datasets.load_iris()\n",
    "        iris_target = data.target\n",
    "        iris_features = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "        train_x, test_x, train_y, test_y = train_test_split(iris_features, iris_target, test_size=0.3, random_state=123)\n",
    "        return train_x, test_x, train_y, test_y\n",
    "    def train_model(self, train_x, train_y):\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(train_x, train_y)\n",
    "        return clf\n",
    "    def proba_data(self, clf, test_x, test_y):\n",
    "        y_predict = clf.predict(test_x)\n",
    "        y_proba = clf.predict_proba(test_x)\n",
    "        accuracy = metrics.accuracy_score(test_y, y_predict) * 100\n",
    "        tot1 = pd.DataFrame([test_y, y_predict]).T\n",
    "        tot2 = pd.DataFrame(y_proba).applymap(lambda x: '%.2f' % x)\n",
    "        tot = pd.merge(tot1, tot2, left_index=True, right_index=True)\n",
    "        tot.columns=['y_true', 'y_predict', 'predict_0', 'predict_1', 'predict_2']\n",
    "        print('The accuracy of Testset is: %d%%' % (accuracy))\n",
    "        print('The result of predict is: \\n', tot.head())\n",
    "        return accuracy, tot\n",
    "    def exc_p(self):\n",
    "        train_x, test_x, train_y, test_y = self.load_data()\n",
    "        clf = self.train_model(train_x, train_y)\n",
    "        res = self.proba_data(clf, test_x, test_y)\n",
    "        return res\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bayes_model().exc_p()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
